Imbalanced classification is primarily challenging as a predictive modelling task because of the severly skewed class distribution.

This is the root cause for poor performance with traditional machine learning models and evaluation metrics that assume a balanced class distribution.

There are additional properties of a classification dataset that are not only challenging for predictive modelling but also increase or compound the difficulty when modelling imbalanced datasets.


Imbalanced classification is specially hard because of the severly skewed class distribution and the unequal misclassification costs.

The difficulty of imbalanced classification is compounded by properties such as dataset size, label noise, and data distribution.

How to develop an intution for the compounding effects on modelling difficulty posed by different dataset properties.


It is common for the majority class to represent a normal case in the domain, whereas the minority class represents an abnormal case, such as fault, fraud, outlier, anomaly, disease state, etc. As such, the interpretation of misclassification errors may differ across the classes.

Misclassifying an example from the majority class as as example from the minority class called a false positive is often not desired, but less critical than classifying an example from the minority class as belonging to the majority class, a so-called false negative.

This is referred to as cost sensitivity of misclassification errors and is a second functional challenge of imbalanced classification.

The two aspects, the "Skewed class distribution and cost sensitivity, are typically referenced when describing the difficulty of imbalanced classification.

Class imbalance was widely acknowledged as a complicating factor for classification. However, some argue that the imbalance ratio is not the only cause for performance degradation in learning from imbalanced data.


Three of the most common problems that affect the performance of the model:

1. Dataset Size
2. Label noise
3. Data Distribution.

It is important to not only acknowledge these properties but to also specifically develop an intution for their impact. This will allow us to select and develop techniques to address them in our own predictive modelling projects.

Understanding these data intrinsic characteristics, as well as their relationship with class imbalance, is crucial for applying existing and developing new techniques to deal with imbalance data.

Compounding effect of Dataset Size:

A problem that often arises in classification is the small number of training instances. This issue, often reported as data rarity or lack of data, is related to the "lack of density" or "insufficiency of information."

For a modest classification task with a balanced class distribution, we might be satisfied with thousands or tens of thousands of examples in order to develop, evaluate, and select a model.


A balanced binary classification with 10000 examples would have 5000 examples of each class. An imbalanced dataset with a 1:100 distribution with the same number of examples would only have 100 examples of the minority class.

As such, the size of the dataset dramatically impacts the imbalanced classification task, and datasets that are thought large in general are, in fact, probably not large enough when working with an imbalanced classification problem.

"Without a sufficient large training set, a classifier may not generalize characteristics of the data. Furthermore, the classifier could also overfit the training data, with a poor performance in out-of-shape test instances."






